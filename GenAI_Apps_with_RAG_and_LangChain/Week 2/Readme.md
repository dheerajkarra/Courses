# Module 2 - RAG using LangChain

In this module, you will learn how to store embeddings using a vector store and how to use Chroma DB to save embeddings. You’ll gain insights into LangChain retrievers like the Vector Store-Based, Multi-Query, Self-Query, and Parent Document Retriever. In hands-on labs, you’ll prepare and preprocess documents for embedding and use watsonx.ai to generate embeddings for your documents. You’ll use vector databases such as Chroma DB and FAISS to store embeddings generated from textual data using LangChain. Finally, you’ll use various retrievers to efficiently extract relevant document segments from text using LangChain.

## Learning Objectives

* Explain how to store embeddings using a vector store.
* Create and configure a vector database to store document embeddings.
* Describe what a LangChain retriever is, and explain the different types of LangChain retrievers.
* Develop a retriever to fetch document segments based on queries.
* Compare the fine-tuning model with RAG.

### Assignment
* Lab: Embed Documents using watsonx’s Embedding Model 
* Lab: Create and Configure a Vector Database to Store Document Embeddings
* Lab: Develop a Retriever to Fetch Document Segments Based on Queries